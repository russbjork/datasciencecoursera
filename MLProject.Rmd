---
title: "Machine Learning Project"
author: "Russ Bjork"
date: "12/26/2020"
output: html_document
---

<body style="background-color:SkyBlue;">

## Environment setup and configuration:  
###     Libraries Silently Loaded
```{r setup, include=FALSE, echo=FALSE}
## 
## Machine Learning Project
##
library(ggplot2)
library(AppliedPredictiveModeling)
library(caret)
library(lattice)
library(dplyr)
library(readr)
library(rattle)
library(randomForest)
library(rpart)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=6, fig.height=4)
```

## Project Overview:
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount   of data about personal activity   relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about   themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people    regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal   will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts   correctly and incorrectly in 5 different ways.  More information is available from the website here:   
http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).    
  
The training data for this project are available here:    
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv   
  
The test data are available here:   
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv   
  
## Data loading and exploration:
The training data set is made of 19622 observations on 160 columns. Many of the columns have NA values or blank values on almost every   observation. The covariants with low or zero variability will be removed because they will not produce any information. The first seven columns   give information about the people who did the test, and also timestamps. The first seven columns will be removed as they add no value to the   models.  
```{r echo=TRUE}
traindata <- read_csv("Data/pml-training.csv")
#head(traindata)
#summary(traindata)
dim(traindata)
#names(traindata)
#table(traindata$classe)

testdata <- read_csv("Data/pml-testing.csv")
#head(testdata)
#summary(traindata)
dim(testdata)
```

## Data cleansing and tiddying:  
All the variables containing NA or blanks are removed from the raw data.  Variables with very low or zero variability are removed as these   columns add no predictive value.  The first seven variables are being removed as they are descriptive of who and when regarding the event data   and serve no purpose in the prediction models.   
```{r echo=TRUE}
remove <- which(colSums(is.na(traindata) | traindata=="")>0.9*dim(traindata)[1]) 
traindata <- traindata[,-remove]
traindata <- traindata[,-c(1:7)]
dim(traindata)
#str(traindata)

remove <- which(colSums(is.na(testdata) | testdata=="")>0.9*dim(testdata)[1]) 
testdata <- testdata[,-remove]
testdata <- testdata[,-c(1:7)]
dim(testdata)
#str(testdata)
```

The tidy data is prepared to be segmented into training and validation data sets, each comprised of 53 columns.  

## Create training and validation sets:  
From the tidy data, 3 data sets will be created.  The first is the training set.  The second in the validation set.  These are both created from   the original training data.  The third is the testing set.  The testing set comes from the original test data.  All sets of data have the   exact same set of cleanup/tidy procedures applied. All sets are comprised of 53 columns/covariants.  
```{r echo=TRUE}
set.seed(12345)
intrain <- createDataPartition(y=traindata$classe, p=0.75, list=FALSE)
training <- traindata[intrain,]
validation <- traindata[-intrain,]
dim(training)
dim(validation)
#summary(validation$classe)
#summary(training$classe)
```

## Build the different models:
In the following sections, we will test 3 different models : * classification tree * random forest * gradient boosting method

In order to limit the effects of overfitting, and improve the efficicency of the models, we will use the *cross-validation technique. We will use 5 folds (usually, 5 or 10 can be used, but 10 folds gives higher run times with no significant increase of the accuracy).
### Classification Tree
```{r echo=TRUE}
control <- trainControl(method="cv", number=5)
model1 <- train(classe~., data=training, method="rpart", trControl=control)
print(model1$finalModel)
fancyRpartPlot(model1$finalModel)
```
### Random Forest
```{r echo=TRUE}
control <- trainControl(method="cv", number=5)
model2 <- train(classe~., data=training, method="rf", trControl=control, verbose=FALSE)
print(model2)
plot(model2, main="Accuracy of Random Forest by Number of Predictors")
plot(model2$finalModel,main="Random Forest Model Error by Number of Trees")
```
### Gradient Boost
```{r echo=TRUE}
control <- trainControl(method="cv", number=5)
model3 <- train(classe~., data=training, method="gbm", trControl=control, verbose=FALSE)
print(model3)
plot(model3, main="Accuracy of Gradient Boost")
```

## Predictions from the different models:  
### Classification Tree
Classification Tree generates an accuracy of 48.8% using cross-validation with 5 steps.  
```{r echo=TRUE}
pred1 <- predict(model1, newdata=validation)
confmat1 <- confusionMatrix(as.factor(validation$classe), pred1)
confmat1$table
confmat1$overall[1]
```
### Random Forest
Random Forest generates an accuracy of 99.4% using cross-validation with 5 steps.  
```{r echo=TRUE}
pred2 <- predict(model2, newdata=validation)
confmat2 <- confusionMatrix(as.factor(validation$classe), pred2)
confmat2$table
confmat2$overall[1]
```
### Gradient Boost
Gradient Boost generates an accuracy of 96.0% using cross-validation with 5 steps.  
```{r echo=TRUE}
pred3 <- predict(model3, newdata=validation)
confmat3 <- confusionMatrix(as.factor(validation$classe), pred3)
confmat3$table
confmat3$overall[1]
```

## Conclusion:  
Based on the prediction outcomes, the Random Forest model (model 2) is the best model. Model 2 will be used to predict the values for "classe" for the test data set.
```{r echo=TRUE}
predtest <- predict(model2,newdata=testdata)
predtest
critvars <- varImp(model2)
critvars
```
